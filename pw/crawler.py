"""crawler with call backs"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_crawler.ipynb.

# %% auto 0
__all__ = ['Callback', 'run_cbs', 'with_cbs', 'Crawl', 'TraveseSameDomainCB', 'ToMDCB']

# %% ../nbs/01_crawler.ipynb 3
from .core import  *
from .helper import *
from operator import attrgetter
import inspect
from fastcore.all import *
import asyncio

# %% ../nbs/01_crawler.ipynb 4
class Callback(): order = 0

async def run_cbs(cbs, method_nm, crawler=None, *args, **kwargs):
    for cb in sorted(cbs, key=attrgetter('order')):
        method = getattr(cb, method_nm, None)
        if method :
            if inspect.iscoroutinefunction(method):
                await method(crawler, *args, **kwargs)
            else:
                method(crawler, *args, **kwargs)

class with_cbs:
    def __init__(self, nm): self.nm = nm
    def __call__(self, f):
        def _f(o, *args, **kwargs):
            try:
                o.callback(f'before_{self.nm}')
                print(self.nm)
                f(o, *args, **kwargs)
                o.callback(f'after_{self.nm}')
            except globals()[f'Cancel{self.nm.title()}Exception']: pass
            finally: o.callback(f'cleanup_{self.nm}')
        return _f

# %% ../nbs/01_crawler.ipynb 6
class Crawl():
    def __init__(self, np: int = 1, to_visit: Optional[List[str]] = None, cbs=None): # type: ignore
        self.np = np
        self.visited = set()
        self.unvisited = set(to_visit)
        self.cbs = L(cbs)
        
    async def one_visit(self, idx):
        page = self.pages[idx]
        
        await self.callback('before_visit',idx)
        
        await page.goto(self.visit_window[idx])
        await page.wait()

        await self.callback('after_visit', idx)
        
    async def run(self, stealth: bool = False, **kwargs):
        async with setup_browser(n=self.np, stealth = stealth, **kwargs) as obj:
            if obj.is_valid:
                    self.pages, self.brow, self.ctx = obj.pages, obj.brow, obj.ctx

                    while self.unvisited:
                        self.visit_window = list(self.unvisited - self.visited)[:self.np] #not visited 
                        if len(self.visit_window) == 0: break
                        tasks = [self.one_visit(i) for i in range(len(self.visit_window)) ]
                    
                        await asyncio.gather(*tasks)  
                        visited_urls = set(self.visit_window)
                        self.unvisited.difference_update(visited_urls) # remove the urls from the to_visit
                        
                        self.visited.update(visited_urls)
                        
    def __getattr__(self, name):
        if name.startswith('before_') or name.startswith('after_'): return partial(self.callback, name)
        raise AttributeError(name)

    async def callback(self, method_nm, *args, **kwargs ): 
        await run_cbs(self.cbs, method_nm, self, *args, **kwargs)          

# %% ../nbs/01_crawler.ipynb 13
class TraveseSameDomainCB(Callback):
    """
    Callback helping traveling all the links available in the same domain.
    """
    def __init__(self, url):
        self.base_domain = domain(url)
        self.order = 1
    
    async def after_visit(self, crawler, idx):
        url = crawler.pages[idx].url
        if domain(url) == self.base_domain:

            links = await find_all_links(crawler.pages[idx])

            if links:
                links = [i for i in links if self.base_domain == domain(i) and not is_same_resource(url, i) ]
                links = {i for i in links if not any ( j in i for j in IGNORE_EXT)}
                links.difference_update(crawler.visited)
                
                crawler.unvisited.update(links)

# %% ../nbs/01_crawler.ipynb 16
class ToMDCB(Callback):
    """
    Callback helping traveling all the links available in the same domain.
    """
    def __init__(self, base_dir="PW"):
        self.order = 2
        self.base_dir = base_dir 
    
    async def after_visit(self, crawler, idx):
        url = crawler.pages[idx].url
        P = Path(f"{self.base_dir}/{url2fn(url)}")
        P.mkdir(exist_ok=True, parents=True)
        fn = P/'index.md'
        print(f"writing to {fn=}")
        md_str = await crawler.pages[idx].h2md()
        #print_md(md_str)
        fn.write_text(md_str)
